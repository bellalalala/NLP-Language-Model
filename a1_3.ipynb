{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all input files\n",
    "import glob\n",
    "read_files = glob.glob(\"C:\\\\Users\\\\81468\\\\OneDrive\\\\Documents\\\\CS6120\\\\homework\\\\a1\\\\train\\\\*.txt\")\n",
    "with open(\"result.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space lines\n",
    "with open('result.txt','r+') as file: \n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file\n",
    "f_open = open(r\"result.txt\",\"r\")\n",
    "data = f_open.read()\n",
    "f_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert punctuations to spaces\n",
    "import re, string\n",
    "re_p = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "data = re_p.sub(' ', data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace duplicate spaces and \\n to single space\n",
    "data = re.sub(r'[\\s]',' ',data) \n",
    "data = re.sub(r' {2,}',' ',data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(s):\n",
    "    c=dict.fromkeys(set(s),0)\n",
    "    for i in range(0,len(s)):\n",
    "        c[s[i]] = c[s[i]]+1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace character that appears <=5 as \"*\"\n",
    "for char in set(data):\n",
    "    if counts[char] <= 5:\n",
    "        data = data.replace(char,'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [10 points]\n",
    "#### Across all ﬁles in the directory (counted together), report the unigram, bigram, and trigram character counts. You can submit them in separate ﬁles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate ngrams\n",
    "def ngram_generate(s,n):\n",
    "    ngram = []\n",
    "    for i in range(len(s)-(n-1)):\n",
    "        ngram.append(s[i:i+n]) \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate bigram and trigram\n",
    "bigram = ngram_generate(data,2)\n",
    "trigram = ngram_generate(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_counts.txt', 'w') as f:\n",
    "    for key, value in unigram_counts.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = count(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('bigram_counts.txt', 'w') as f:\n",
    "    for key, value in bigram_counts.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts = count(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_counts.txt', 'w') as f:\n",
    "    for key, value in trigram_counts.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [10 points]\n",
    "#### Calculate the perplexity for each ﬁle in the test set using linear interpolation smoothing method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing tr_set and hd_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data by sentence\n",
    "import random\n",
    "with open(\"result.txt\", \"r\") as f:\n",
    "    data_ttl = f.read().split('.')\n",
    "random.seed(1)\n",
    "random.shuffle(data_ttl)\n",
    "tr_set = data_ttl[:int(len(data_ttl)*.80)]\n",
    "hd_set = data_ttl[int(len(data_ttl)*.80)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert punctuations to spaces\n",
    "tr_set = re_p.sub(' ', '.'.join(tr_set)) \n",
    "hd_set = re_p.sub(' ', '.'.join(hd_set))\n",
    "#replace duplicate spaces and \\n to single space\n",
    "tr_set = re.sub(r'[\\s]',' ',tr_set) \n",
    "tr_set = re.sub(r' {2,}',' ',tr_set) \n",
    "hd_set = re.sub(r'[\\s]',' ',hd_set) \n",
    "hd_set = re.sub(r' {2,}',' ',hd_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_new = count(tr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace character that appears <=5 as \"*\"\n",
    "for char in set(tr_set):\n",
    "    if counts_new[char] <= 5:\n",
    "        tr_set = tr_set.replace(char,'*')\n",
    "for char in set(hd_set):\n",
    "    if counts_new[char] <= 5 or char not in counts_new:\n",
    "        hd_set = hd_set.replace(char,'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate probability on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(ngram_counts):\n",
    "    prob_ngram = dict.fromkeys(ngram_counts)\n",
    "    for key in ngram_counts.keys():\n",
    "        prob_ngram[key] = ngram_counts[key]/sum(ngram_counts.values())\n",
    "    return prob_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prob(a,b,n):\n",
    "    prob_ngram = dict.fromkeys(a)\n",
    "    for key in a:\n",
    "        prob_ngram[key] = a[key]/b[key[0:n-1]]\n",
    "    return prob_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ngrams and count ngrams on training set\n",
    "tr_unigram_counts = count(tr_set)\n",
    "tr_bigram = ngram_generate(tr_set,2)\n",
    "tr_bigram_counts = count(tr_bigram)\n",
    "tr_trigram = ngram_generate(tr_set,3)\n",
    "tr_trigram_counts = count(tr_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the probability\n",
    "tr_prob_unigram = unigram_prob(tr_unigram_counts)\n",
    "tr_prob_bigram = ngram_prob(tr_bigram_counts,tr_unigram_counts,2)\n",
    "tr_prob_trigram = ngram_prob(tr_trigram_counts,tr_bigram_counts,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create lambda combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambdas\n",
    "l = []\n",
    "for i in range(1,9):\n",
    "    for j in range(1,10-i):\n",
    "        l.append([i/10, j/10, 1-i/10-j/10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate prob on held out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_trigram = ngram_generate(hd_set,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the interpolation estimated probability for each trigram\n",
    "#format of pro_hat: [{tri1:p1_1,tri2:p2_1,...},{tri1:p1_2,tri2:p2_2,...},...]\n",
    "pro_hat = []\n",
    "for i in range(len(l)):\n",
    "    pro_hat.append(dict.fromkeys(hd_trigram))\n",
    "    for key in hd_trigram:\n",
    "        if key in tr_prob_trigram.keys():\n",
    "            pro_hat[i][key] = l[i][0]*tr_prob_trigram[key] + l[i][1]*tr_prob_bigram[key[-2:]] + l[i][2]*tr_prob_unigram[key[-1]]\n",
    "        elif key[-2:] in tr_prob_bigram.keys():\n",
    "            pro_hat[i][key] = l[i][1]*tr_prob_bigram[key[-2:]] + l[i][2]*tr_prob_unigram[key[-1]]\n",
    "        else:\n",
    "            pro_hat[i][key] = l[i][2]*tr_prob_unigram[key[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate log(p(hd_set))\n",
    "import math\n",
    "log_p_hd = []\n",
    "for j in range(len(l)):\n",
    "    p=0\n",
    "    for i in range(len(hd_set)-3):\n",
    "        p = p + math.log(pro_hat[j][hd_set[i:i+3]])\n",
    "    log_p_hd.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.1, 0.09999999999999995]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best lambda combination\n",
    "l[log_p_hd.index(max(log_p_hd))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best lambda combination is 0.8 for trigram, 0.1 for bigram and 0.1 for unigram, as it gives the highest probability on held out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate probability on total training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_unigram = unigram_prob(unigram_counts)\n",
    "prob_bigram = ngram_prob(bigram_counts,unigram_counts,2)\n",
    "prob_trigram = ngram_prob(trigram_counts,bigram_counts,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "read_files = glob.glob(\"C:\\\\Users\\\\81468\\\\OneDrive\\\\Documents\\\\CS6120\\\\homework\\\\a1\\\\test_data\\\\*\")\n",
    "testdata = []\n",
    "for i in range(len(read_files)):\n",
    "    with open(read_files[i], \"rb\") as infile:\n",
    "        for line in infile:\n",
    "            if not line.isspace():\n",
    "                testdata.append(infile.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert punctuations to spaces\n",
    "#replace duplicate spaces and \\n to single space\n",
    "#replace character that appears <=5 as \"*\"\n",
    "for i in range(len(testdata)):\n",
    "    testdata[i] = re_p.sub(' ', testdata[i]) \n",
    "    testdata[i] = re.sub(r'[\\s]',' ',testdata[i]) \n",
    "    testdata[i] = re.sub(r' {2,}',' ',testdata[i]) \n",
    "    for char in set(testdata[i]):\n",
    "        if (char in counts and counts[char] <= 5) or char not in counts:\n",
    "            testdata[i] = testdata[i].replace(char,'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each test file, calculate the estimated trigram prob and perplexity \n",
    "test_trigram = []\n",
    "test_pp = []\n",
    "test_pro_hat=[]\n",
    "for i in range(len(testdata)):\n",
    "    test_trigram.append(ngram_generate(testdata[i],3))\n",
    "    test_pro_hat.append(dict.fromkeys(test_trigram[i]))\n",
    "    for key in test_trigram[i]:\n",
    "        if key in prob_trigram.keys():\n",
    "            test_pro_hat[i][key] = 0.8*prob_trigram[key] + 0.1*prob_bigram[key[-2:]] + 0.1*prob_unigram[key[-1]]\n",
    "        elif key[-2:] in prob_bigram.keys():\n",
    "            test_pro_hat[i][key] = 0.1*prob_bigram[key[-2:]] + 0.1*prob_unigram[key[-1]]\n",
    "        else:\n",
    "            test_pro_hat[i][key] = 0.1*prob_unigram[key[-1]]        \n",
    "    pp = 0\n",
    "    for j in range(len(testdata[i])-3):\n",
    "        pp = pp + math.log(1/test_pro_hat[i][testdata[i][j:j+3]])                       \n",
    "    perp = math.exp(pp/(len(testdata[i])-3))\n",
    "    test_pp.append(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_report = {}\n",
    "for i in range(len(test_pp)):\n",
    "    perp_report[read_files[i][-4:]] = test_pp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perp_report_intp.txt', 'w') as f:\n",
    "    for key, value in perp_report.items():\n",
    "        f.write('%s, %s \\n' % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "largest = nlargest(50, test_pp)\n",
    "largest_pfile = {}\n",
    "for i in range(0,50):\n",
    "    largest_pfile[read_files[test_pp.index(largest[i])][-4:]] = largest[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('largest_pfile_intp.txt', 'w') as f:\n",
    "    for key, value in largest_pfile.items():\n",
    "        f.write('%s, %s \\n' % (key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After check, they are all French files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [10 points]\n",
    "#### Build another language model with add-λ smoothing (use λ = 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 0.1 smoothing\n",
    "tri={}\n",
    "V = len(unigram_counts)\n",
    "uniq = list(set(unigram_counts.keys()))\n",
    "for i in range(len(unigram_counts)):\n",
    "    for j in range(len(unigram_counts)):\n",
    "        for k in range(len(unigram_counts)):\n",
    "            k3 = uniq[i]+uniq[j]+uniq[k]\n",
    "            k2 = uniq[i]+uniq[j]\n",
    "            if k3 in trigram_counts:\n",
    "                tri[k3] = (trigram_counts[k3]+0.1)/(bigram_counts[k2]+0.1*V)\n",
    "            elif k2 in bigram_counts:\n",
    "                tri[k3] = 0.1/(bigram_counts[k2]+0.1*V)\n",
    "            else:\n",
    "                tri[k3] = 1/V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each test file, calculate the perplexity \n",
    "import math\n",
    "test_pp_add1 = []\n",
    "for i in range(len(testdata)):\n",
    "    p = 0\n",
    "    for j in range(len(testdata[i])-3):\n",
    "        p = p + math.log(1/tri[testdata[i][j:j+3]])                           \n",
    "    perp = math.exp(p/(len(testdata[i])-3))\n",
    "    test_pp_add1.append(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_report_add1 = {}\n",
    "for i in range(len(test_pp_add1)):\n",
    "    perp_report_add1[read_files[i][-4:]] = test_pp_add1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perp_report_addk.txt', 'w') as f:\n",
    "    for key, value in perp_report_add1.items():\n",
    "        f.write('%s, %s \\n' % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "largest_add1 = nlargest(50, test_pp_add1)\n",
    "largest_pfile_add1 = {}\n",
    "for i in range(0,50):\n",
    "    largest_pfile_add1[read_files[test_pp_add1.index(largest_add1[i])][-4:]] = largest_add1[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('largest_pfile_addk.txt', 'w') as f:\n",
    "    for key, value in largest_pfile_add1.items():\n",
    "        f.write('%s, %s \\n' % (key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After check, they are all French files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
